type = 'l', xlab = '', ylab = ''), scale = slider(0.1, 2, step = 0.1))
set.seed(44)
x <- rcauchy(n)
manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = ''), scale = slider(0.1, 2, step = 0.1))
set.seed(45)
x <- rcauchy(n)
manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = ''), scale = slider(0.1, 2, step = 0.1))
set.seed(46)
x <- rcauchy(n)
manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = ''), scale = slider(0.1, 2, step = 0.1))
set.seed(47)
x <- rcauchy(n)
manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = ''), scale = slider(0.1, 2, step = 0.1))
set.seed(48)
x <- rcauchy(n)
manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = ''), scale = slider(0.1, 2, step = 0.1))
set.seed(9)
x <- rcauchy(n)
manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = ''), scale = slider(0.1, 2, step = 0.1))
# Cauchy log-likelilhood
n <- 25; scale <- 1
set.seed(9)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-5, 5, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = '')
# Cauchy log-likelilhood
n <- 25; scale <- 1
set.seed(9)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = '')
set.seed(47)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
type = 'l', xlab = '', ylab = '')
# Cauchy log-likelilhood
n <- 25; scale <- 0.01
set.seed(47)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '')
# Cauchy log-likelilhood
n <- 25; scale <- 0.01
set.seed(47)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-5, 5, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '')
set.seed(9)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-5, 5, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '')
set.seed(47)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-5, 5, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '')
# Cauchy log-likelilhood
n <- 25; scale <- 0.01
set.seed(47)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-5, 5, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-5, 5, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x * scale) / scale)^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-5, 5, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
set.seed(9)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
set.seed(9)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
points(x, 0, pch = 4)
points(x, rep(0, length(x)), pch = 4)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
points(x, rep(-6, length(x)), pch = 4)
set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
points(x, rep(-5.5, length(x)), pch = 4)
set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
points(x, rep(-5.9, length(x)), pch = 4)
set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
points(x, rep(-5.95, length(x)), pch = 4)
set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x))^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
points(x, rep(-6, length(x)), pch = 4)
set.seed(47)
# set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-10, 10, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
# set.seed(3)
x <- rcauchy(n)
# manipulate(plot(y <- seq(-100, 100, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - scale * x) / scale)^2))}),
#      type = 'l', xlab = '', ylab = ''), scale = slider(0.5, 2, step = 0.1))
plot(y <- seq(-10, 10, length.out = 1e+4), sapply(y, function(z){-log(pi * scale) - mean(log(1 + ((z - x) / scale)^2))}),
type = 'l', xlab = '', ylab = '', main = 'Plot of the Cauchy log-likelihood')
points(x, rep(-10.5, length(x)), pch = 4)
source("~/Library/CloudStorage/Dropbox/Research/convex_loss_estimation/Code/SplineLinearReg.R")
str(mtcars)
str(cars)
lm(mpg ~ wt, data = mtcars)
lm_cars <- lm(mpg ~ wt, data = mtcars)
plot(lm_cars)
plot(wt, mpg, pch = 4, type = 'p', data = mtcars)
plot(mtcars$wt, mtcars$mpg, pch = 4, type = 'p')
abline(cars_lm, lwd = 2, col = 2)
abline(lm_cars, lwd = 2, col = 2)
plot(mtcars$wt, mtcars$mpg, pch = 4, type = 'p', xlab = 'wt', ylab = 'mpg')
abline(lm_cars, lwd = 2, col = 2)
update(lm_cars, . ~ . + hp)
summary(lm_cars)
lm_cars <- update(lm_cars, . ~ . + hp)
summary(lm_cars)
setwd("~/Dropbox/Research/convex_loss_estimation/Code")
sink("cars.txt")
summary(lm_cars)
sink()
plot(lm_cars)
oldpar = par()
par(mfrow = 2, mfcol = 2)
par(mfrow = c(2,2))
plot(lm_cars)
plot(x <- seq(-5, 5, by = 0.01), huber(x, K = 2), type = 'l', col = '2', lwd = 2, xlim = c(-5, 5), ylim = c(-0.2, 25/2),
xlab = '', ylab = '', axes = FALSE, xaxs = 'i', yaxs = 'i')
axis(1, at = c(-6, -2, 2, 6), pos = 0, labels = c("", "-K", "K", ""))
segments(0, 0, 0, 25/2)
# abline(h = 0)
abline(v = -2, lty = 3)
abline(v = 2, lty = 3)
# axis(2, labels = F, pos = 0)
arrows(5 - 0.1, 0, 5, 0, angle = 30, length = 0.1)  # Arrow for x-axis
arrows(0, 25/2 - 0.1, 0, 25/2, angle = 30, length = 0.1)  # Arrow for y-axis
# axis(2)
lines(x, x^2/2, type = 'l', col = '4')
legend(x = -5.2, y = 2, legend = c("Huber loss", "Squared error loss"), cex = 0.9, lty = c(1, 1), col = c(2, 4), bty = 'n')
setwd("~/Dropbox/Research/convex_loss_estimation/Code/Figures")
# Population-level plots
plot_fisher(dist = "cauchy")
# Plot (true and estimated) decreasing score functions, together with the corresponding loss functions and densities
plot_fisher <- function(scorefun = NULL, dist = FALSE, plot.main = TRUE, k = 1000, debug = FALSE, alpha = 2, sigma = 2, save = TRUE, filename = NULL, ...) {
if (debug) {
save <- FALSE; browser()
}
kde <- NULL
empirical <- (class(scorefun) == "scorefun") # whether or not the score function is estimated from data
if (empirical) list2env(scorefun, envir = environment())
else stopifnot("scorefun object or named distribution required as input" = !isFALSE(dist))
if (save & is.null(filename)) filename <- ifelse(is.null(scorefun), dist, paste0(dist, "-data"))
main <- NULL # plot title
## J plot:
if (save) pdf(file = paste0(filename, "-J.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- "(a) Plot of J and its least concave majorant"
# main <- expression(bold(paste("Plot of ", J, " and ", hat(J))))
legend <- col <- NULL # legend text and colours
u <- 0:k / k # grid for the J plot
J0 <- J0hat <- Jhat <- NULL
# Population-level J_0 and its LCM for some specific densities
if (dist == "gaussian") J0 <- J0hat <- dnorm(qnorm(u))
else if (dist == "cauchy") {
J0 <- (1 - cos(2*pi*u)) / (2*pi)
lcm0 <- gcmlcm(u, J0, type = "lcm")
J0hat <- approxfun(lcm0$x.knots, lcm0$y.knots)(u)
# t0 <- newton(function(t) t * sin(t) + cos(t) - 1, 2)$root; x0 <- cot(t0/2)
# segments(0, 0, t0 / (2 * pi), (1 - cos(t0)) / (2 * pi), col = 3)
# segments(1, 0, 1 - t0 / (2 * pi), (1 - cos(t0)) / (2 * pi), col = 3)
}
else if (dist == "pareto") {
stopifnot(alpha > 0 & sigma > 0)
J0 <- alpha * (2 * pmin(u, 1-u))^(1 + 1/alpha) / (2 * sigma)
J0hat <- alpha * (2 * pmin(u, 1-u)) / (2 * sigma)
}
# LCM based on an estimated J:
if (empirical) {
Jhat <- approxfun(lcm$x.knots, lcm$y.knots)(u)
legend <- c("J", expression(hat(J))); col <- c(1, 4)
matplot(u, cbind(J, Jhat), col = c(1, 4), lty = c(1, 1), type = "l", ylim = 1.1 * range(c(Jhat, J0hat)), xlab = "u", ylab = "J", main = main, ...)
} else plot(range(u), 1.1 * range(J0), type = "n", xlab = "u", ylab = "J", main = main, ...)
if (!is.null(J0)) {
matlines(u, cbind(J0, J0hat), col = c(2, 3), lty = c(1, 1), lwd = c(2, 2))
legend <- c(legend, expression(J[0]), expression(hat(J)[0])); col <- c(col, 2, 3)
}
legend("topright", legend = legend, col = col, lty = rep(1, 4), ...)
if (save) dev.off()
# Default R colours: 1 - black, 2 - red, 3 - green, 4 - blue, 5 - cyan, 6 - magenta, 7 - yellow, 8 - grey
## Score function (hat(psi)) plot:
if (save) pdf(file = paste0(filename, "-score.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- ("(b) Score function plot")
grid1 <- psi0 <- psihat <- legend <- col <- NULL
# Define an appropriate grid (xlim) and legend
if (empirical) {
legend <- "Estimated score"; col <- 1
if (dist == "cauchy") {
grid1 <- grid[abs(grid) < 3]; psihat <- psi[abs(grid) < 3]
} else {
grid1 <- grid; psihat <- psi
}
} else {
if (dist == "pareto") {
grid1 <- (10^(min(16, 1/alpha)) - 1) * sigma * seq(-1, 1, length.out = k)
} else grid1 <- seq(-5, 5, length.out = k)
}
# True score functions and their antitonic projections
if (dist == "gaussian") {
psi0 <- -grid1; legend <- c(legend, "Gaussian score"); col <- c(col, 2)
} else if (dist == "cauchy") {
psi0 <- t(psi_cauchy(grid1)); legend <- c(legend, "Cauchy score", "Projected Cauchy score"); col <- c(col, 2, 3)
} else if (dist == "pareto") {
psi0 <- cbind(-(alpha + 1) * sign(grid1) / (abs(grid1) + sigma) , -alpha * sign(grid1) / sigma);
legend <- c(legend, paste0("Pareto", "(", alpha, ",", sigma, ") score"), "Projected Pareto score"); col <- c(col, 2, 3)
}
# Plot score functions
plot(range(grid1), 1.1 * range(c(psi0, psihat)), type = "n", xlab = "x", ylab = expression(psi), main = main, ...)
if (empirical) lines(grid1, psihat, type = "l")
if (!is.null(psi0)) matlines(grid1, psi0, type = "l", col = c(2, 3), lty = c(1, 1), lwd = c(2, 2))
legend("topright", legend = legend, col = col, lty = rep(1, 4), ...)
if (save) dev.off()
## Loss function (-phi) plot:
if (save) pdf(file = paste0(filename, "-loss.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- ("(c) Loss function plot")
legend <- col <- l0 <- loss <- NULL
if (empirical) {
legend <- c("Estimated convex loss"); col <- 1; loss <- -phi
} else {
if (dist == "gaussian") grid <- seq(-5, 5, length.out = k)
if (dist == "cauchy") grid <- seq(-9, 9, length.out = k)
if (dist == "pareto") grid <- (10^(min(16, 1/alpha)) - 1) * sigma * seq(-1, 1, length.out = k)
}
# Population-level loss functions:
if (dist == "gaussian") {
l0 <- -log(dnorm(grid)); legend <- c(legend, "Squared error loss"); col <- c(col, 2)
} else if (dist == "cauchy") {
l0 <- -t(phi_cauchy(grid)); legend <- c(legend, "-log(Cauchy density)", "Projected Cauchy loss"); col <- c(col, 2, 3)
} else if (dist == "pareto") {
l0 <- -log(cbind(alpha * sigma^alpha / (2 * (abs(grid) + sigma)^(alpha + 1)), # true density
alpha * exp(-alpha * abs(grid) / sigma) / (2 * sigma))) # Fisher projection
legend <- c(legend, paste0("-log(Pareto", "(", alpha, ",", sigma, ")", " density)"), "Fisher-projected loss"); col <- c(col, 2, 3)
if (alpha > 1) {
l0 <- cbind(l0, -log((alpha - 1) * exp(-(alpha - 1) * abs(grid) / sigma) / (2 * sigma))) # log-concave MLE
legend <- c(legend, "ML-projected loss"); col <- c(col, 4)
}
}
# Plot loss functions
plot(range(grid), 1.1 * range(c(l0, loss)), type = "n", xlab = "x", ylab = "y", main = main, ...)
if (empirical) lines(grid, loss, type = "l")
if (!is.null(l0)) matlines(grid, l0, type = "l", col = c(2, 3, 4), lty = c(1, 1))
legend("bottomright", legend = legend, col = col, lty = rep(1, 5), ...)
if (save) dev.off()
## Density plot:
if (save) pdf(file = paste0(filename, "-density.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- ("(d) Density plot")
legend <- col <- p0 <- NULL
if (!is.null(l0)) p0 <- exp(-l0)
if (empirical) {
# legend <- c("Estimated log-concave density", "KDE"); col <- c(1, 6)
legend <- c("Estimated log-concave density"); col <- 1
} else pdf <- NULL
# Use the same grid as for the loss function plot
# True densities and their projections given by exp(-l0)
if (dist == "gaussian") {
legend <- c(legend, "Gaussian density"); col <- c(col, 2)
} else if (dist == "cauchy") {
legend <- c(legend, "Cauchy density", "Fisher projection"); col <- c(col, 2, 3)
} else if (dist == "pareto") {
legend <- c(legend, paste0("Pareto", "(", alpha, ",", sigma, ")", " density"), "Fisher projection"); col <- c(col, 2, 3)
if (alpha > 1) {
legend <- c(legend, "Log-concave ML projection"); col <- c(col, 4)
}
}
# Plot densities
plot(range(grid), 1.1 * range(c(p0, pdf)), type = "n", xlab = "x", ylab = "y", yaxs = "i", main = main, ...)
if (empirical) {
lines(grid, pdf)
# lines(kde, col = 6)
}
if (!is.null(l0)) matlines(grid, p0, type = "l", col = c(2, 3, 4), lty = rep(1, 3))
legend("topright", legend = legend, col = col, lty = rep(1, 6), ...)
if (save) dev.off()
# ## KDE plot (based on data):
# if (empirical) {
#   if (save) pdf(file = paste0(filename, "-kde.pdf"), width = 8.2, height = 5.4)
#   if (plot.main) main <- "Kernel density estimate"
#   plot(kde, xlim = c(min(grid), max(grid)), main = main)
#   if (save) dev.off()
# }
}
# Population-level plots
plot_fisher(dist = "cauchy")
# Plot (true and estimated) decreasing score functions, together with the corresponding loss functions and densities
plot_fisher <- function(scorefun = NULL, dist = FALSE, plot.main = TRUE, k = 1000, debug = FALSE, alpha = 2, sigma = 2, save = TRUE, filename = NULL, ...) {
if (debug) {
save <- FALSE; browser()
}
kde <- NULL
empirical <- (class(scorefun) == "scorefun") # whether or not the score function is estimated from data
if (empirical) list2env(scorefun, envir = environment())
else stopifnot("scorefun object or named distribution required as input" = !isFALSE(dist))
if (save & is.null(filename)) filename <- ifelse(is.null(scorefun), dist, paste0(dist, "-data"))
main <- NULL # plot title
## J plot:
if (save) pdf(file = paste0(filename, "-J.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- "(a) Plot of J and its least concave majorant"
# main <- expression(bold(paste("Plot of ", J, " and ", hat(J))))
legend <- col <- NULL # legend text and colours
u <- 0:k / k # grid for the J plot
J0 <- J0hat <- Jhat <- NULL
# Population-level J_0 and its LCM for some specific densities
if (dist == "gaussian") J0 <- J0hat <- dnorm(qnorm(u))
else if (dist == "cauchy") {
J0 <- (1 - cos(2*pi*u)) / (2*pi)
lcm0 <- gcmlcm(u, J0, type = "lcm")
J0hat <- approxfun(lcm0$x.knots, lcm0$y.knots)(u)
# t0 <- newton(function(t) t * sin(t) + cos(t) - 1, 2)$root; x0 <- cot(t0/2)
# segments(0, 0, t0 / (2 * pi), (1 - cos(t0)) / (2 * pi), col = 3)
# segments(1, 0, 1 - t0 / (2 * pi), (1 - cos(t0)) / (2 * pi), col = 3)
}
else if (dist == "pareto") {
stopifnot(alpha > 0 & sigma > 0)
J0 <- alpha * (2 * pmin(u, 1-u))^(1 + 1/alpha) / (2 * sigma)
J0hat <- alpha * (2 * pmin(u, 1-u)) / (2 * sigma)
}
# LCM based on an estimated J:
if (empirical) {
Jhat <- approxfun(lcm$x.knots, lcm$y.knots)(u)
legend <- c("J", expression(hat(J))); col <- c(1, 4)
matplot(u, cbind(J, Jhat), col = c(1, 4), lty = c(1, 1), type = "l", ylim = 1.1 * range(c(Jhat, J0hat)), xlab = "u", ylab = "J", main = main, ...)
} else plot(range(u), 1.1 * range(J0), type = "n", xlab = "u", ylab = "J", main = main, ...)
if (!is.null(J0)) {
matlines(u, cbind(J0, J0hat), col = c(2, 3), lty = c(1, 1), lwd = c(2, 2))
legend <- c(legend, expression(J[0]), expression(hat(J)[0])); col <- c(col, 2, 3)
}
legend("topright", legend = legend, col = col, lty = rep(1, 4), ...)
if (save) dev.off()
# Default R colours: 1 - black, 2 - red, 3 - green, 4 - blue, 5 - cyan, 6 - magenta, 7 - yellow, 8 - grey
## Score function (hat(psi)) plot:
if (save) pdf(file = paste0(filename, "-score.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- ("(b) Score function plot")
grid1 <- psi0 <- psihat <- legend <- col <- NULL
# Define an appropriate grid (xlim) and legend
if (empirical) {
legend <- "Estimated score"; col <- 1
if (dist == "cauchy") {
grid1 <- grid[abs(grid) < 3]; psihat <- psi[abs(grid) < 3]
} else {
grid1 <- grid; psihat <- psi
}
} else {
if (dist == "pareto") {
grid1 <- (10^(min(16, 1/alpha)) - 1) * sigma * seq(-1, 1, length.out = k)
} else grid1 <- seq(-5, 5, length.out = k)
}
# True score functions and their antitonic projections
if (dist == "gaussian") {
psi0 <- -grid1; legend <- c(legend, "Gaussian score"); col <- c(col, 2)
} else if (dist == "cauchy") {
psi0 <- t(psi_cauchy(grid1)); legend <- c(legend, "Cauchy score", "Projected Cauchy score"); col <- c(col, 2, 3)
} else if (dist == "pareto") {
psi0 <- cbind(-(alpha + 1) * sign(grid1) / (abs(grid1) + sigma) , -alpha * sign(grid1) / sigma);
legend <- c(legend, paste0("Pareto", "(", alpha, ",", sigma, ") score"), "Projected Pareto score"); col <- c(col, 2, 3)
}
# Plot score functions
plot(range(grid1), 1.1 * range(c(psi0, psihat)), type = "n", xlab = "x", ylab = expression(psi), main = main, ...)
if (empirical) lines(grid1, psihat, type = "l")
if (!is.null(psi0)) matlines(grid1, psi0, type = "l", col = c(2, 3), lty = c(1, 1), lwd = c(2, 2))
legend("topright", legend = legend, col = col, lty = rep(1, 4), ...)
if (save) dev.off()
## Loss function (-phi) plot:
if (save) pdf(file = paste0(filename, "-loss.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- ("(c) Loss function plot")
legend <- col <- l0 <- loss <- NULL
if (empirical) {
legend <- c("Estimated convex loss"); col <- 1; loss <- -phi
} else {
if (dist == "gaussian") grid <- seq(-5, 5, length.out = k)
if (dist == "cauchy") grid <- seq(-9, 9, length.out = k)
if (dist == "pareto") grid <- (10^(min(16, 1/alpha)) - 1) * sigma * seq(-1, 1, length.out = k)
}
# Population-level loss functions:
if (dist == "gaussian") {
l0 <- -log(dnorm(grid)); legend <- c(legend, "Squared error loss"); col <- c(col, 2)
} else if (dist == "cauchy") {
l0 <- -t(phi_cauchy(grid)); legend <- c(legend, "-log(Cauchy density)", "Projected Cauchy loss"); col <- c(col, 2, 3)
} else if (dist == "pareto") {
l0 <- -log(cbind(alpha * sigma^alpha / (2 * (abs(grid) + sigma)^(alpha + 1)), # true density
alpha * exp(-alpha * abs(grid) / sigma) / (2 * sigma))) # Fisher projection
legend <- c(legend, paste0("-log(Pareto", "(", alpha, ",", sigma, ")", " density)"), "Fisher-projected loss"); col <- c(col, 2, 3)
if (alpha > 1) {
l0 <- cbind(l0, -log((alpha - 1) * exp(-(alpha - 1) * abs(grid) / sigma) / (2 * sigma))) # log-concave MLE
legend <- c(legend, "ML-projected loss"); col <- c(col, 4)
}
}
# Plot loss functions
plot(range(grid), 1.1 * range(c(l0, loss)), type = "n", xlab = "x", ylab = "y", main = main, ...)
if (empirical) lines(grid, loss, type = "l")
if (!is.null(l0)) matlines(grid, l0, type = "l", col = c(2, 3, 4), lty = c(1, 1), lwd = c(2, 2))
legend("bottomright", legend = legend, col = col, lty = rep(1, 5), ...)
if (save) dev.off()
## Density plot:
if (save) pdf(file = paste0(filename, "-density.pdf"), width = 8.2, height = 5.4)
if (plot.main) main <- ("(d) Density plot")
legend <- col <- p0 <- NULL
if (!is.null(l0)) p0 <- exp(-l0)
if (empirical) {
# legend <- c("Estimated log-concave density", "KDE"); col <- c(1, 6)
legend <- c("Estimated log-concave density"); col <- 1
} else pdf <- NULL
# Use the same grid as for the loss function plot
# True densities and their projections given by exp(-l0)
if (dist == "gaussian") {
legend <- c(legend, "Gaussian density"); col <- c(col, 2)
} else if (dist == "cauchy") {
legend <- c(legend, "Cauchy density", "Fisher projection"); col <- c(col, 2, 3)
} else if (dist == "pareto") {
legend <- c(legend, paste0("Pareto", "(", alpha, ",", sigma, ")", " density"), "Fisher projection"); col <- c(col, 2, 3)
if (alpha > 1) {
legend <- c(legend, "Log-concave ML projection"); col <- c(col, 4)
}
}
# Plot densities
plot(range(grid), 1.1 * range(c(p0, pdf)), type = "n", xlab = "x", ylab = "y", yaxs = "i", main = main, ...)
if (empirical) {
lines(grid, pdf)
# lines(kde, col = 6)
}
if (!is.null(l0)) matlines(grid, p0, type = "l", col = c(2, 3, 4), lty = rep(1, 3), lwd = c(2, 2))
legend("topright", legend = legend, col = col, lty = rep(1, 6), ...)
if (save) dev.off()
# ## KDE plot (based on data):
# if (empirical) {
#   if (save) pdf(file = paste0(filename, "-kde.pdf"), width = 8.2, height = 5.4)
#   if (plot.main) main <- "Kernel density estimate"
#   plot(kde, xlim = c(min(grid), max(grid)), main = main)
#   if (save) dev.off()
# }
}
# Population-level plots
plot_fisher(dist = "cauchy")
